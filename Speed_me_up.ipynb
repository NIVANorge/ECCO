{
 "metadata": {
  "name": "",
  "signature": "sha256:3e99f1850de86606803efa49d7e9d401251f293188aa7f3d3dcdcfdba2068558"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Read functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import sys, time, os, json\n",
      "from netCDF4 import Dataset  \n",
      "from matplotlib.path import Path\n",
      "from matplotlib import cm\n",
      "import matplotlib.patches as patches\n",
      "from matplotlib import path\n",
      "from matplotlib.transforms import Bbox\n",
      "from math import pi, cos, sin, radians, atan, asin\n",
      "import mpl_toolkits.basemap.pyproj as pyproj\n",
      "import matplotlib.pyplot as plt\n",
      "from multiprocessing import Process,Pool,cpu_count\n",
      "import os\n",
      "import time\n",
      "import time as clock\n",
      "\n",
      "from ECCO_functions import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Lake Processing for individual processor"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def MT_Means_Over_Lake(nc_path, lake_file, lake_num, outputprefix, threeD=True, tt=None):\n",
      "    '''Purpose   -  This program is the main wrapper to execute the ECCO project functions. It is \n",
      "    designed to be executed within the MultiProcessing module. Where one Lake is processed by one Process.\n",
      "    \n",
      "    Inputs   -\n",
      "    nc_path is the file path for the CORDEX NetCDF file\n",
      "    lake_data is file path and filename to GeoJSON file\n",
      "    lake_num is the lake number to be processed (the int feature number from within the lake_data file)\n",
      "    outputprefix is the directory for the outputs (it will be then nested according to netcdf file name)\n",
      "    \n",
      "    Outputs - compressed text files with a time-series of values weighted averageds over each lake \n",
      "    \n",
      "    Note : Multiprocessing is not shared memory, so need to load data for each process\n",
      "    '''\n",
      "    a = clock.time()                                                            #Time program started\n",
      "    \n",
      "    num = lake_num\n",
      "    orog = Height_CORDEX()                                                       # Get orography data\n",
      "    EB_id, lake_path, lake_altitude = Read_LakesV2(lake_file)\n",
      "    clim_dat,rlat,rlon,time,metadata,txtfname = Read_CORDEX_V2(nc_path)          # Get NetCDF4 data\n",
      "    vname, m1, m2, dexp, m3, m4, m5, m6, drange_orignial = metadata              # Split metadata\n",
      "    \n",
      "    b = clock.time()                                                             # Time record\n",
      "    \n",
      "    lake_cart = Path_Lake_and_Islands(num = num,lake_path=lake_path)\n",
      "    lake_rprj = Path_Reproj(lake_cart,False)             \n",
      "    sub_clim,sub_rlat,sub_rlon = TrimToLake(lake_rprj,clim_dat[0,:,:],rlat,\n",
      "                                            rlon,off = 3, show = False)\n",
      "    weight_mask = Pixel_Weights(lake_rprj,sub_clim,sub_rlat,sub_rlon)\n",
      "    sub_orog,sub_rlat,sub_rlon = TrimToLake(lake_rprj,orog[:,:],rlat,\n",
      "                                            rlon,off = 3, show = False)           # Subset area around lake\n",
      "    hght,offset = Orographic_Adjustment(weight_mask,sub_orog,\n",
      "                                        lake_altitude[num],clim_dat,chatty=False) # Height adjustment\n",
      "    c = clock.time()                                                              # Time record\n",
      "    \n",
      "    if threeD:\n",
      "        sub_clim,sub_rlat,sub_rlon = TrimToLake3D(lake_rprj,clim_dat[:,:,:],rlat,rlon,off = 3, show = False)\n",
      "        tlist = Weighted_Mean_3D(weight_mask, sub_clim, chatty=False)\n",
      "    else:\n",
      "        tlist =[]\n",
      "        if tt is None:\n",
      "            tt = clim_dat.shape[0]\n",
      "        for t in xrange(tt):\n",
      "        #for t in xrange(len(time)):\n",
      "            sub_clim,sub_rlat,sub_rlon = TrimToLake(lake_rprj,clim_dat[t,:,:],rlat,rlon,off = 3, show = False)\n",
      "            final_val = Weighted_Mean(weight_mask,sub_clim,chatty=False)\n",
      "            tlist.append(final_val)\n",
      "            #print 'Timestep:',t, '  Weighted temperature =','%6.2f'%(final_val) #-272.15),'Deg C'\n",
      "        tlist = np.array(tlist)\n",
      "    \n",
      "    d = clock.time()                                                                 # Record time    \n",
      "    \n",
      "    idnew = EB_id[lake_num][2:]                                                      # EB-lake ID (stripped a bit)  \n",
      "    fnm_head = vname+'_'                                                             # Code of the CORDEX variable\n",
      "    hcreate = 'Height offset = %f  Data = %s, Time range = %s  Scenario = %s'%(\\\n",
      "                offset, clim_dat.long_name, drange_orignial, dexp)\n",
      "    Folder_Create(outputprefix,fnm_head,EB_id[lake_num])                            # Create a folder system\n",
      "    pathname = os.path.join(outputprefix, '_'.join([m1, m2, m3, m4, m5, m6]),\n",
      "                            idnew[:2], idnew[:4], idnew)\n",
      "    #print(pathname)\n",
      "    if not os.path.exists(pathname): os.makedirs(pathname)\n",
      "    \n",
      "    np.savetxt(os.path.join(pathname, txtfname),\n",
      "               tlist,fmt='%7.3f',newline='\\n', header=hcreate)  \n",
      "\n",
      "    \n",
      "    e = clock.time()\n",
      "    print ('%4.2f sec : Read Data:'%(b-a))\n",
      "    print ('%4.2f sec : Calculated height offset and Weighting Mask'%(c-b))\n",
      "    print ('%4.2f sec : Weighted time-series'%(d-c))\n",
      "    print ('%4.2f sec : Folder path and file creation'%(d-c))\n",
      "    print ('%4.2f sec : Total'%(e-a))\n",
      "    \n",
      "    return"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Test run"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nc_path = '/uio/kant/geo-metos-u1/blaken/Downloads/tas_EUR-11_ICHEC-EC-EARTH_rcp85_r3i1p1_DMI-HIRHAM5_v1_day_20060101-20101231.nc'\n",
      "lake_data = 'Lakes/largest100.geojson'\n",
      "lake_num = 54\n",
      "out_path = 'Outputs'\n",
      "\n",
      "#MT_Means_Over_Lake(nc_path, lake_data, lake_num, out_path, threeD=False, tt=30) # Shorter time period, and slow method\n",
      "\n",
      "MT_Means_Over_Lake(nc_path, lake_data, lake_num, out_path) # All time period, fast method"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.82 sec : Read Data:\n",
        "0.96 sec : Calculated height offset and Weighting Mask\n",
        "5.50 sec : Weighted time-series\n",
        "5.50 sec : Folder path and file creation\n",
        "8.30 sec : Total\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Below is the code i removed from the MT function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "    #nc_fname = os.path.basename(nc_path)\n",
      "    #metadata = os.path.splitext(nc_fname)[0].split('_')\n",
      "    # # metadata is a list of cordex netcdf file metadata\n",
      "    #vname, m1, m2, dexp, m3, m4, m5, m6, drange_orignial = metadata    # dexp is driving experiment\n",
      "    # #print(metadata)\n",
      "    #drange = '_'.join([dexp, drange_orignial])\n",
      "    #txtfname = '_'.join([vname, drange])  # vname + '_' + drange # '%s_%s' % (vname, drange)\n",
      "        \n",
      "    #nc = Dataset(nc_path)           # Read the netcdf Data \n",
      "    #clim_dat = nc.variables[vname]\n",
      "    #rlat = nc.variables['rlat']\n",
      "    #rlon = nc.variables['rlon']\n",
      "    #time = nc.variables['time']\n",
      "    \n",
      "    #print 'Offset to be applied (in Kelvin):',offset\n",
      "    \n",
      "    #hcreate = 'Height offset = '+ '%6.2f'%offset+'  Data = '+clim_dat.long_name+', time range of file = '+drange+' Model = '+dexp\n",
      "    \n",
      "    \n",
      "    #out = os.path.join('Outputs', fnm_head)       \n",
      "    #ideb = EB_id[lake_num][2:]\n",
      "    #print(ideb)\n",
      "    #l = len(ideb)\n",
      "    #if l == 6:\n",
      "    #    idnew = ideb\n",
      "    #elif l == 5:\n",
      "    #    idnew = '0%s' % ideb\n",
      "    #elif l == 4:\n",
      "    #    idnew = '00%s' % ideb\n",
      "    #elif l == 3:\n",
      "    #    idnew = '000%s' % ideb\n",
      "    #elif l == 2:\n",
      "    #    idnew = '0000%s' % ideb\n",
      "    #elif l == 1:\n",
      "    #    idnew = '00000%s' % ideb\n",
      "    \n",
      "    \n",
      "    #out = '/uio/kant/geo-metos-u1/blaken/Work/Python/ECCO/Outputs/'+fnm_head\n",
      "    # txtfname = Gen_FileName(out,lake_name[num],drange,'_3D.txt.gz' if threeD else '.txt.gz')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    }
   ],
   "metadata": {}
  }
 ]
}