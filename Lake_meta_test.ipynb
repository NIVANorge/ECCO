{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and development for catchment and lake weight calclations ###\n",
    "This was needed after modifying the way the catchment weights and metadata were caluclated,\n",
    "improving it. Also, I had to fix a bug in the weighting function. So I updated the lake functions when I created the catchment functions also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ECCO_functions_v2 as ECCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import osgeo.ogr\n",
    "import sys, time, os, json, glob\n",
    "import numpy as np\n",
    "import pyproj\n",
    "import h5py\n",
    "import csv \n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Lake_surfaceweights_meta(nc_path,sbar=False):\n",
    "    '''\n",
    "    Purpose:          \n",
    "    This program Generates metadata files used to speed up the final runs.\n",
    "    This was forthe lakes (not catchments), and created a metadata text\n",
    "    file, to be used as a mini-database in pandas.\n",
    "    This is the second version if of a program to generate lake surface \n",
    "    weights and metadata. This was made after the program to do the same\n",
    "    for the catchments. This creates hdf5 weight file, and .csv metadata.\n",
    "    '''\n",
    "        \n",
    "    clim_dat,rlat,rlon,time,metadata,txtfname = ECCO.Read_CORDEX_V2(nc_path)\n",
    "    vname, m1, m2, dexp, m3, m4, m5, m6, drange_orignial = metadata \n",
    " \n",
    "    var_type = clim_dat.standard_name       # What kind of CORDEX data?\n",
    "    dat_loaded = clim_dat[0,:,:]            # Load CORDEX data into RAM\n",
    "    print np.shape(dat_loaded)\n",
    "    rlat_loaded = rlat[:]\n",
    "    rlon_loaded = rlon[:]\n",
    "\n",
    "    lake_file = 'Lakes/ecco-biwa_lakes_v.0.2.shp'\n",
    "    \n",
    "    thefilename = 'lake_weights'\n",
    "    FILE= 'Lakes/Weights/' + thefilename +'.h5'                # Set up HDF5 file output\n",
    "    if os.path.isfile(FILE):\n",
    "        #print 'HDF5 File already exists. Leaving loop so you dont clobber it by accident.'\n",
    "        #print 'To run this function, decide manually if you want to remove it or not.'\n",
    "        #return\n",
    "        print 'hdf weights file exists, removing it...'\n",
    "        os.remove(FILE)\n",
    "    else:\n",
    "        print 'Creating file: ',FILE\n",
    "    fweights = h5py.File(FILE,'w')\n",
    "    \n",
    "    # set and write header info for the metadata file\n",
    "    metacsv = 'Lakes/Metadata/Lake_meta.csv'\n",
    "    if os.path.isfile(metacsv) == True:\n",
    "        print 'Earlier metadata exists. Erasing it...'\n",
    "        os.remove(metacsv)\n",
    "    tmplist = ['num','EB_id','area','npix','ypix','xpix'] \n",
    "    ECCO.write_metadata_csv(mfname=metacsv,meta_list=tmplist)\n",
    "    \n",
    "    #orog = ECCO.Height_CORDEX()\n",
    "    ShapeData = osgeo.ogr.Open(lake_file)\n",
    "    TheLayer = ShapeData.GetLayer(iLayer=0)\n",
    "    dolakes=range(TheLayer.GetFeatureCount())\n",
    "\n",
    "    if sbar:\n",
    "        icnt = 0\n",
    "    \n",
    "    for num in dolakes[0:1000]:\n",
    "        feature1 = TheLayer.GetFeature(num) \n",
    "        lake_feature = feature1.ExportToJson(as_object=True)\n",
    "        lake_cart = ECCO.Path_LkIsl_ShpFile(lake_feature['geometry']['coordinates']) \n",
    "        EB_id = lake_feature['properties']['EBhex'][2:]\n",
    "        lake_altitude=lake_feature['properties']['vfp_mean']\n",
    "\n",
    "        lake_rprj = ECCO.Path_Reproj(lake_cart,False)\n",
    "\n",
    "        sub_clim,sub_rlat,sub_rlon = ECCO.TrimToLake(lake_rprj,dat_loaded,rlat_loaded,\n",
    "                                                        rlon_loaded,off = 3, show = False) \n",
    "        weight_mask = ECCO.Pixel_Weights(lake_rprj,sub_clim,sub_rlat,sub_rlon)\n",
    "\n",
    "\n",
    "        pix_truth = (weight_mask > 0.0)      # Count how many \n",
    "        pxnum = len(weight_mask[pix_truth])  #  pixels of data are needed.\n",
    "    \n",
    "        ypix = -99\n",
    "        xpix = -99\n",
    "        if pxnum == 1:\n",
    "            xxx,yyy = ECCO.Get_LatLonLim(lake_rprj.vertices)  # Find upp./low.lake lims.\n",
    "            ypix = (ECCO.Closest(rlat,yyy[0]))                # For lakes of one pixel  \n",
    "            xpix = (ECCO.Closest(rlon,xxx[0]))\n",
    "        if pxnum < 1:\n",
    "            pxnum = 1  # Small bug where it thinks lakes dont exist, no biggy...\n",
    "        if pxnum > 1:\n",
    "            ECCO.Write_HDF_weights(fw=fweights,EB_id=EB_id,weights=weight_mask) \n",
    "        \n",
    "        tmpmeta =[num,EB_id, ECCO.Area_Lake_and_Islands(lake_cart),pxnum,ypix,xpix]\n",
    "        ECCO.write_metadata_csv(mfname=metacsv,meta_list=tmpmeta)\n",
    "        \n",
    "        #print num,EB_id, ECCO.Area_Lake_and_Islands(lake_cart),pxnum,ypix,xpix\n",
    "        if sbar:\n",
    "            icnt=icnt+1\n",
    "            if (float(icnt) % 10.) == 0.0:\n",
    "                ECCO.Update_Progress(float(icnt)/len(dolakes))\n",
    "    fweights.close()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(412, 424)\n",
      "hdf weights file exists, removing it...\n",
      "Earlier metadata exists. Erasing it...\n",
      "Progress: [----------------------------------------] 0% "
     ]
    }
   ],
   "source": [
    "#ncfile= '/uio/kant/geo-metos-u1/blaken/datadisk/ECCO/CORDEX/Data_CORDEX/tas_EUR-11_ICHEC-EC-EARTH_historical_r1i1p1_KNMI-RACMO22E_v1_day_19660101-19701231.nc'\n",
    "ncfile ='CORDEX/tas_EUR-11_ICHEC-EC-EARTH_rcp45_r1i1p1_KNMI-RACMO22E_v1_day_20960101-21001231.nc'\n",
    "\n",
    "Lake_surfaceweights_meta(nc_path=ncfile,sbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Now use the test metadata to create a new routine for main processing##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ccc59c'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lake_meta = pd.read_csv('Lakes/Metadata/Lake_meta.csv')\n",
    "lake_meta = lake_meta.set_index('EB_id')\n",
    "lake_meta.index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lake_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599f21 1.01356\n",
      "854aec 1.01046\n",
      "1ccbad 1.02334\n",
      "a96b25 1.01708\n",
      "60bfab 1.01972\n",
      "Found problems in 5.0  lakes\n"
     ]
    }
   ],
   "source": [
    "# Example of reading the data from a hdf5 file after creation using the metdatada as a lookup\n",
    "#filein = 'Catchments/Weights/catchment_weights.h5'\n",
    "#f = h5py.File(filein,\"r\")\n",
    "# Going to test the weighting for the lakes (.npy) files, and also for the catchment hdf5 file.\n",
    "# will read them in a loop, and sum the pixels. Will stop if the weight masks are not equal to\n",
    "# 1 at any time.\n",
    "\n",
    "acnt = 0\n",
    "for n in lake_meta.index:\n",
    "    if lake_meta.npix[n] > 1:\n",
    "        with h5py.File('Lakes/Weights/lake_weights.h5','r') as fp:\n",
    "            tmparray = fp[n]\n",
    "            tmparray = tmparray[:,:]\n",
    "        if tmparray.sum() > 1.01:\n",
    "            acnt += 1.\n",
    "            print n, tmparray.sum()\n",
    "print 'Found problems in', acnt,' lakes'\n",
    "\n",
    "# Found problems in 2026.0  lakes: too little weight\n",
    "# Found problems in 2072.0  lakes: too much weight\n",
    "# After change: Found problems in 0  lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time as clock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Fast_v4(nc_path, lake_file, outputprefix,lstart=0,lstop=275265,\n",
    "                       hexlist=None,tt=None,plots = False,rprt=False,sbar=False,\n",
    "                       rprt_loop=False):\n",
    "    '''\n",
    "    Input:\n",
    "    \n",
    "           hexlist      If not None, this must be a list of hexcodes which match lake codes.\n",
    "                        These codes will be the list of lakes to be processed, with\n",
    "                        (regardless of lstart / lstop settings). The list should be ascii values\n",
    "                        in any order. E.g. hexlist = ['a2204','155980','d23e4a','7aa917']\n",
    "    \n",
    "     k.w.agrs:   \n",
    "    \n",
    "           plots        True or False(default). If True, preview plots are created. I reccomend\n",
    "                        using this feature carefully. Unless you have set a very small number of\n",
    "                        lakes, this will produce a huge number of plots!\n",
    "    \n",
    "           rprt         Returns information on how long the program took to load the data,\n",
    "                        complete, and how many lakes were processed.\n",
    "        \n",
    "           rprt_loop    Returns info on how long each specific lake took (can be a lot...)\n",
    "    \n",
    "           sbar         Create a status bar, to show the progression through a large loop. Not\n",
    "                        shown by default. As, when this pro is on MPI it is not a good feature.\n",
    "    \n",
    "     Notes:       Out of 275265 total lakes, 264532 of them are within one pixel of EUR-11 data.\n",
    "                  Metadata called from lake hexcode as index: not shapefile feature number.\n",
    "    '''\n",
    "    if rprt:\n",
    "        atime = clock.time()\n",
    "\n",
    "    #lk_processed_inf = pd.read_csv('Metadata/Meta_Lakes.csv')  # Load metadata\n",
    "    lake_meta = pd.read_csv('Lakes/Metadata/Lake_meta.csv') # Load the metadata\n",
    "    lake_meta = lake_meta.set_index('EB_id')                # Index with EB_id\n",
    "    \n",
    "    ShapeData = osgeo.ogr.Open(lake_file)                  # Make a link to Lake Shape Files\n",
    "    TheLayer = ShapeData.GetLayer(iLayer=0)\n",
    "    \n",
    "    clim_dat,rlat,rlon,timeCDX,metadata,txtfname = ECCO.Read_CORDEX_V2(nc_path) # CORDEX Read\n",
    "    vname, m1, m2, dexp, m3, m4, m5, m6, drange_orignial = metadata     # Metadata from fname\n",
    "    var_type = clim_dat.standard_name                              # What kind of CORDEX data?\n",
    "    dat_loaded = clim_dat[:,:,:]                                   # Load CORDEX data\n",
    "    rlat_loaded = rlat[:]\n",
    "    rlon_loaded = rlon[:]\n",
    "    \n",
    "    orog = ECCO.Height_CORDEX()                                 # EUR-11 surface height data \n",
    "    \n",
    "    if hexlist == None:                   # Set up the list of lakes to process:\n",
    "        dolakes=np.arange(lstart,lstop,1) #If no Hexcodes, use lstart/lstop to form a list\n",
    "    else:\n",
    "        dolakes= lake_meta.num[test]      #Else gen. list of nums from hex code meta\n",
    "    \n",
    "    thefilename = 'Lakes_'+str.split(nc_path,'/')[-1][:-3]\n",
    "\n",
    "    FILE= outputprefix + thefilename +'.h5'       # Set up HDF5 file output\n",
    "    if os.path.isfile(FILE):\n",
    "        print 'Earlier file already exists: Overwriting...'\n",
    "        os.remove(FILE)\n",
    "    else:\n",
    "        print 'No file found. Creating: ',FILE\n",
    "    f = h5py.File(FILE,'w')\n",
    "\n",
    "    # LOOP OVER ALL LAKES (or specified lakes from lstart to lstop)\n",
    "    if rprt:\n",
    "        btime = clock.time()\n",
    "    if sbar:\n",
    "        icnt = 0\n",
    "    for n in dolakes[27:33]:\n",
    "        tlist = []\n",
    "        feature1 = TheLayer.GetFeature(n)           # Get individ. lake in shapefile\n",
    "        lake_feature = feature1.ExportToJson(as_object=True)\n",
    "        lake_cart = ECCO.Path_LkIsl_ShpFile(lake_feature['geometry']['coordinates'])\n",
    "        lake_altitude=lake_feature['properties']['stf_mean']\n",
    "        EB_id = lake_feature['properties']['EBhex']\n",
    "        EB_id = EB_id[2:]                           # Strip off the hexcode label 0x\n",
    "        lake_rprj = ECCO.Path_Reproj(lake_cart,False)    # Reproj. lake to CORDEX plr. rotated\n",
    "        if EB_id != lake_meta.index[n]:      # Some handy error check\n",
    "            print 'Warning! Lake feature and metadata miss-match for some reason. Check it out:'\n",
    "            print 'Problem at:',num,lake_meta.num[n],EB_id,lake_meta.index[n]\n",
    "        if plots:     \n",
    "            ECCO.Preview_Lake(lake_cart)        \n",
    "            print 'Area in km^2 (not inc. islands):', ECCO.Area_Lake_and_Islands(lake_cart),         \n",
    "            print ', No. xy bound. points:',len(lake_cart.vertices)\n",
    "        if lake_meta.npix[EB_id] == 1:         # ONE PIXEL LAKES <<<\n",
    "            ypix = lake_meta.ypix[EB_id]       # Get the pre-calc. pixel indexes...\n",
    "            xpix = lake_meta.xpix[EB_id]       # ...calc in MT_Gen_SWeights() earlier\n",
    "            if lake_altitude == None:                 # Some lakes don't have alitude values\n",
    "                offset = -999.\n",
    "            else:\n",
    "                offset = ECCO.OnePix_HOffset(lake_altitude,orog[ypix, xpix],var_type)\n",
    "            tlist = dat_loaded[:, ypix, xpix]\n",
    "            if plots:\n",
    "                plt.plot(tlist)\n",
    "        else:         # LAKES OF MORE THAN ONE PIXEL \n",
    "            with h5py.File('Lakes/Weights/lake_weights.h5','r') as fp:\n",
    "                tmparray = fp[EB_id]\n",
    "                weight_mask = tmparray[:,:]      # Load the precalculated weight data\n",
    "            \n",
    "            sub_clim,sub_rlat,sub_rlon = ECCO.TrimToLake3D(lake_rprj,dat_loaded,rlat_loaded,\n",
    "                                                      rlon_loaded,\n",
    "                                                      off = 3, show = False)\n",
    "            \n",
    "            if ((var_type == 'air_temperature')| (var_type == 'surface_air_pressure')): \n",
    "                sub_orog,sub_rlat,sub_rlon = ECCO.TrimToLake(lake_rprj,orog,rlat_loaded,\n",
    "                                                            rlon_loaded,off = 3, show = False)\n",
    "                if lake_altitude == None:       # Some lakes don't have alitude values\n",
    "                    offset = -999.\n",
    "                else:\n",
    "                    hght,offset = ECCO.Orographic_Adjustment(weight_mask,sub_orog,\n",
    "                                                        lake_altitude,clim_dat,chatty=False)\n",
    "            else:\n",
    "                hght = -999.         # If no altitude data then \n",
    "                offset = -999.       # set height and offset to missing vals\n",
    "            \n",
    "            \n",
    "            tlist = ECCO.Weighted_Mean_3D(weight_mask, sub_clim, chatty=False)  # Here's the t-series\n",
    "            \n",
    "            if plots:\n",
    "                ECCO.Show_LakeAndData(lake_rprj,dat_loaded[0,:,:],rlat,rlon,zoom=3.)\n",
    "                ECCO.Preview_Weights(lake_rprj,weight_mask,sub_rlat,sub_rlon)\n",
    "                plt.plot(tlist)\n",
    "                \n",
    "        if rprt_loop:\n",
    "            print '\\rStats:',(float(n)/float(lstop))*100.,'% ',n,EB_id,offset,lake_altitude\n",
    "        if sbar:\n",
    "            icnt=icnt+1\n",
    "            if (float(icnt) % 10.) == 0.0:\n",
    "                ECCO.Update_Progress(float(icnt)/float(len(dolakes)-1))\n",
    "\n",
    "        ECCO.Write_HDF(f,EB_id,tlist,offset,lake_meta.area[n])  # Write inside function\n",
    "        feature1=0\n",
    "        lake_feature = 0\n",
    "\n",
    "    f.close()                                # Close the HDF5 file after the lake loop finishes\n",
    "    #subprocess.call([\"gzip\", FILE])          # Compress the file and remove original with gzip             \n",
    "    if rprt:\n",
    "            print '\\nTime to read data: %4.2f sec'%(btime - atime)\n",
    "            print 'Time to Process %i lakes: %4.2f sec'%(len(dolakes),clock.time() - btime)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file found. Creating:  Lakes_tas_EUR-11_ICHEC-EC-EARTH_rcp45_r1i1p1_KNMI-RACMO22E_v1_day_20960101-21001231.h5\n",
      "Progress: [########################################] 100% \n",
      "\n",
      "Time to read data: 29.19 sec\n",
      "Time to Process 10 lakes: 0.21 sec\n"
     ]
    }
   ],
   "source": [
    "ncfile = 'CORDEX/tas_EUR-11_ICHEC-EC-EARTH_rcp45_r1i1p1_KNMI-RACMO22E_v1_day_20960101-21001231.nc'\n",
    "\n",
    "lkfile = 'Lakes/ecco-biwa_lakes_v.0.2.shp'\n",
    "\n",
    "ECCO.Fast_v4(nc_path=ncfile, lake_file=lkfile, outputprefix='',lstart=0,lstop=10,\n",
    "             plots = False,rprt=True,sbar=True,rprt_loop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lake_meta = pd.read_csv('Lakes/Metadata/Lake_meta.csv')\n",
    "lake_meta = lake_meta.set_index('EB_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42f085 0.998918 1.0\n",
      "43f298 0.995141 1.0\n",
      "c78c76 0.999786 1.0\n",
      "f73adf 0.998658 1.0\n",
      "c8d0ed 0.998443 1.0\n"
     ]
    }
   ],
   "source": [
    "# Example of reading the data from a hdf5 file after creation using the metdatada as a lookup\n",
    "#filein = 'Catchments/Weights/catchment_weights.h5'\n",
    "#f = h5py.File(filein,\"r\")\n",
    "# Going to test the weighting for the lakes (.npy) files, and also for the catchment hdf5 file.\n",
    "# will read them in a loop, and sum the pixels. Will stop if the weight masks are not equal to\n",
    "# 1 at any time.\n",
    "\n",
    "acnt = 0\n",
    "for n in lake_meta.index[200:250]:\n",
    "    if lake_meta.npix[n] > 1:\n",
    "        with h5py.File('Lakes/Weights/lake_weights.h5','r') as fp:\n",
    "            tmparray = fp[n]\n",
    "            tmparray = tmparray[:,:]\n",
    "            if tmparray.sum() != 1.:\n",
    "                print n, tmparray.sum(),\n",
    "                #pix_weights /= pix_weights.sum()\n",
    "                tmparray /= tmparray.sum()\n",
    "                print tmparray.sum()\n",
    "                #plt.imshow(tmparray,interpolation='none',cmap=plt.cm.gray)\n",
    "                #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with h5py.File('Lakes/Weights/lake_weights.h5','r') as fp:\n",
    "    tmparray = fp['c8d0ed']\n",
    "    tmparray = tmparray[:,:]\n",
    "    tmparray /= tmparray.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99844325"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atest = tmparray != 0.\n",
    "tmparray[atest]\n",
    "np.sum(tmparray[atest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
