{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now I need to recalculate lake metadata (weights ect) in the same manner as the catchments\n",
    "import ECCO_functions_v2 as ECCO\n",
    "\n",
    "import pandas as pd\n",
    "import osgeo.ogr\n",
    "import sys, time, os, json, glob\n",
    "import numpy as np\n",
    "import pyproj\n",
    "import h5py\n",
    "import csv  # <<-- dont forget to add this ti function file\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Lake_surfaceweights_meta(nc_path,sbar=False):\n",
    "    '''\n",
    "    Purpose:          \n",
    "    This program Generates metadata files used to speed up the final runs.\n",
    "    This was forthe lakes (not catchments), and created a metadata text\n",
    "    file, to be used as a mini-database in pandas.\n",
    "    This is the second version if of a program to generate lake surface \n",
    "    weights and metadata. This was made after the program to do the same\n",
    "    for the catchments. This creates hdf5 weight file, and .csv metadata.\n",
    "    '''\n",
    "        \n",
    "    clim_dat,rlat,rlon,time,metadata,txtfname = ECCO.Read_CORDEX_V2(nc_path)\n",
    "    vname, m1, m2, dexp, m3, m4, m5, m6, drange_orignial = metadata \n",
    " \n",
    "    var_type = clim_dat.standard_name       # What kind of CORDEX data?\n",
    "    dat_loaded = clim_dat[0,:,:]            # Load CORDEX data into RAM\n",
    "    print np.shape(dat_loaded)\n",
    "    rlat_loaded = rlat[:]\n",
    "    rlon_loaded = rlon[:]\n",
    "\n",
    "    lake_file = 'Lakes/ecco-biwa_lakes_v.0.2.shp'\n",
    "    \n",
    "    thefilename = 'lake_weights'\n",
    "    FILE= 'Lakes/Weights/' + thefilename +'.h5'                # Set up HDF5 file output\n",
    "    if os.path.isfile(FILE):\n",
    "        #print 'HDF5 File already exists. Leaving loop so you dont clobber it by accident.'\n",
    "        #print 'To run this function, decide manually if you want to remove it or not.'\n",
    "        #return\n",
    "        print 'hdf weights file exists, removing it...'\n",
    "        os.remove(FILE)\n",
    "    else:\n",
    "        print 'Creating file: ',FILE\n",
    "    fweights = h5py.File(FILE,'w')\n",
    "    \n",
    "    # set and write header info for the metadata file\n",
    "    metacsv = 'Lakes/Metadata/Lake_meta.csv'\n",
    "    if os.path.isfile(metacsv) == True:\n",
    "        print 'Earlier metadata exists. Erasing it...'\n",
    "        os.remove(metacsv)\n",
    "    tmplist = ['num','EB_id','area','npix','ypix','xpix'] \n",
    "    ECCO.write_metadata_csv(mfname=metacsv,meta_list=tmplist)\n",
    "    \n",
    "    #orog = ECCO.Height_CORDEX()\n",
    "    ShapeData = osgeo.ogr.Open(lake_file)\n",
    "    TheLayer = ShapeData.GetLayer(iLayer=0)\n",
    "    dolakes=range(TheLayer.GetFeatureCount())\n",
    "\n",
    "    if sbar:\n",
    "        icnt = 0\n",
    "    \n",
    "    for num in dolakes[0:1000]:\n",
    "        feature1 = TheLayer.GetFeature(num) \n",
    "        lake_feature = feature1.ExportToJson(as_object=True)\n",
    "        lake_cart = ECCO.Path_LkIsl_ShpFile(lake_feature['geometry']['coordinates']) \n",
    "        EB_id = lake_feature['properties']['EBhex'][2:]\n",
    "        lake_altitude=lake_feature['properties']['vfp_mean']\n",
    "\n",
    "        lake_rprj = ECCO.Path_Reproj(lake_cart,False)\n",
    "\n",
    "        sub_clim,sub_rlat,sub_rlon = ECCO.TrimToLake(lake_rprj,dat_loaded,rlat_loaded,\n",
    "                                                        rlon_loaded,off = 3, show = False) \n",
    "        weight_mask = ECCO.Pixel_Weights(lake_rprj,sub_clim,sub_rlat,sub_rlon)\n",
    "\n",
    "\n",
    "        pix_truth = (weight_mask > 0.0)      # Count how many \n",
    "        pxnum = len(weight_mask[pix_truth])  #  pixels of data are needed.\n",
    "    \n",
    "        ypix = -99\n",
    "        xpix = -99\n",
    "        if pxnum == 1:\n",
    "            xxx,yyy = ECCO.Get_LatLonLim(lake_rprj.vertices)  # Find upp./low.lake lims.\n",
    "            ypix = (ECCO.Closest(rlat,yyy[0]))                # For lakes of one pixel  \n",
    "            xpix = (ECCO.Closest(rlon,xxx[0]))\n",
    "        if pxnum < 1:\n",
    "            pxnum = 1  # Small bug where it thinks lakes dont exist, no biggy...\n",
    "        if pxnum > 1:\n",
    "            ECCO.Write_HDF_weights(fw=fweights,EB_id=EB_id,weights=weight_mask) \n",
    "        \n",
    "        tmpmeta =[num,EB_id, ECCO.Area_Lake_and_Islands(lake_cart),pxnum,ypix,xpix]\n",
    "        ECCO.write_metadata_csv(mfname=metacsv,meta_list=tmpmeta)\n",
    "        \n",
    "        #print num,EB_id, ECCO.Area_Lake_and_Islands(lake_cart),pxnum,ypix,xpix\n",
    "        if sbar:\n",
    "            icnt=icnt+1\n",
    "            if (float(icnt) % 10.) == 0.0:\n",
    "                ECCO.Update_Progress(float(icnt)/len(dolakes))\n",
    "    fweights.close()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(412, 424)\n",
      "hdf weights file exists, removing it...\n",
      "Earlier metadata exists. Erasing it...\n",
      "Progress: [----------------------------------------] 0% "
     ]
    }
   ],
   "source": [
    "#ncfile= '/uio/kant/geo-metos-u1/blaken/datadisk/ECCO/CORDEX/Data_CORDEX/tas_EUR-11_ICHEC-EC-EARTH_historical_r1i1p1_KNMI-RACMO22E_v1_day_19660101-19701231.nc'\n",
    "ncfile ='CORDEX/tas_EUR-11_ICHEC-EC-EARTH_rcp45_r1i1p1_KNMI-RACMO22E_v1_day_20960101-21001231.nc'\n",
    "\n",
    "Lake_surfaceweights_meta(nc_path=ncfile,sbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ccc59c'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lake_meta = pd.read_csv('Lakes/Metadata/Lake_meta.csv')\n",
    "lake_meta = lake_meta.set_index('EB_id')\n",
    "lake_meta.index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lake_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599f21 1.01356\n",
      "854aec 1.01046\n",
      "1ccbad 1.02334\n",
      "a96b25 1.01708\n",
      "60bfab 1.01972\n",
      "Found problems in 5.0  lakes\n"
     ]
    }
   ],
   "source": [
    "# Example of reading the data from a hdf5 file after creation using the metdatada as a lookup\n",
    "#filein = 'Catchments/Weights/catchment_weights.h5'\n",
    "#f = h5py.File(filein,\"r\")\n",
    "# Going to test the weighting for the lakes (.npy) files, and also for the catchment hdf5 file.\n",
    "# will read them in a loop, and sum the pixels. Will stop if the weight masks are not equal to\n",
    "# 1 at any time.\n",
    "\n",
    "acnt = 0\n",
    "for n in lake_meta.index:\n",
    "    if lake_meta.npix[n] > 1:\n",
    "        with h5py.File('Lakes/Weights/lake_weights.h5','r') as fp:\n",
    "            tmparray = fp[n]\n",
    "            tmparray = tmparray[:,:]\n",
    "        if tmparray.sum() > 1.01:\n",
    "            acnt += 1.\n",
    "            print n, tmparray.sum()\n",
    "print 'Found problems in', acnt,' lakes'\n",
    "\n",
    "# Found problems in 2026.0  lakes: too little weight\n",
    "# Found problems in 2072.0  lakes: too much weight\n",
    "# After change: Found problems in 0  lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
