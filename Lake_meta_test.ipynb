{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now I need to recalculate lake metadata (weights ect) in the same manner as the catchments\n",
    "import ECCO_functions_v2 as ECCO\n",
    "\n",
    "import pandas as pd\n",
    "import osgeo.ogr\n",
    "import sys, time, os, json, glob\n",
    "import numpy as np\n",
    "import pyproj\n",
    "import h5py\n",
    "import csv  # <<-- dont forget to add this ti function file\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MT_Gen_SWeights(nc_path):\n",
    "    '''\n",
    "    Purpose:          \n",
    "    This program Generates metadata files used to speed up the final runs.\n",
    "    This was forthe lakes (not catchments), and created a metadata text\n",
    "    file, to be used as a mini-database in pandas.\n",
    "    '''\n",
    "        \n",
    "    clim_dat,rlat,rlon,time,metadata,txtfname = ECCO.Read_CORDEX_V2(nc_path)\n",
    "    vname, m1, m2, dexp, m3, m4, m5, m6, drange_orignial = metadata \n",
    " \n",
    "    var_type = clim_dat.standard_name       # What kind of CORDEX data?\n",
    "    dat_loaded = clim_dat[0,:,:]            # Load CORDEX data into RAM\n",
    "    print np.shape(dat_loaded)\n",
    "    rlat_loaded = rlat[:]\n",
    "    rlon_loaded = rlon[:]\n",
    "\n",
    "    lake_file = 'Lakes/ecco-biwa_lakes_v.0.2.shp'\n",
    "    \n",
    "    thefilename = 'lake_weights'\n",
    "    FILE= 'Lakes/Weights/' + thefilename +'.h5'                # Set up HDF5 file output\n",
    "    if os.path.isfile(FILE):\n",
    "        #print 'HDF5 File already exists. Leaving loop so you dont clobber it by accident.'\n",
    "        #print 'To run this function, decide manually if you want to remove it or not.'\n",
    "        #return\n",
    "        print 'hdf weights file exists, removing it...'\n",
    "        os.remove(FILE)\n",
    "    else:\n",
    "        print 'Creating file: ',FILE\n",
    "    fweights = h5py.File(FILE,'w')\n",
    "    \n",
    "    #orog = ECCO.Height_CORDEX()\n",
    "    ShapeData = osgeo.ogr.Open(lake_file)\n",
    "    TheLayer = ShapeData.GetLayer(iLayer=0)\n",
    "    dolakes=range(TheLayer.GetFeatureCount())\n",
    "    #alist = []\n",
    "    #blist = []\n",
    "    for num in dolakes[0:10]:\n",
    "        feature1 = TheLayer.GetFeature(num) \n",
    "        lake_feature = feature1.ExportToJson(as_object=True)\n",
    "        lake_cart = ECCO.Path_LkIsl_ShpFile(lake_feature['geometry']['coordinates']) \n",
    "        EB_id = lake_feature['properties']['EBhex'][2:]\n",
    "        lake_altitude=lake_feature['properties']['vfp_mean']\n",
    "\n",
    "        lake_rprj = ECCO.Path_Reproj(lake_cart,False)\n",
    "\n",
    "        sub_clim,sub_rlat,sub_rlon = ECCO.TrimToLake(lake_rprj,dat_loaded,rlat_loaded,\n",
    "                                                        rlon_loaded,off = 3, show = False) \n",
    "        weight_mask = ECCO.Pixel_Weights(lake_rprj,sub_clim,sub_rlat,sub_rlon)\n",
    "\n",
    "\n",
    "        pix_truth = (weight_mask > 0.0)    # Count how many times the weight mask is\n",
    "        pxnum = len(weight_mask[pix_truth])  #  above 0.0 (i.e. how many pixels of data are needed)\n",
    "    \n",
    "        etime = 1.0  # Nothing, can be removed (in the meta too, but be careful to change the later programs)\n",
    "\n",
    "        #print EB_id,' Weight mask sum:',np.sum(weight_mask)\n",
    "        #alist.append(EB_id)\n",
    "        #blist.append(np.sum(weight_mask))\n",
    "        \n",
    "        ypix = -99\n",
    "        xpix = -99\n",
    "        if pxnum == 1:\n",
    "            xxx,yyy = ECCO.Get_LatLonLim(lake_rprj.vertices)  # Find upp./low.lake lims.\n",
    "            ypix = (ECCO.Closest(rlat,yyy[0]))                # For lakes of one pixel  \n",
    "            xpix = (ECCO.Closest(rlon,xxx[0]))\n",
    "        if pxnum < 1:\n",
    "            pxnum = 1  # Small bug where it thinks lakes dont exist, no biggy...\n",
    "        if pxnum > 1:\n",
    "            ECCO.Write_HDF_weights(fw=fweights,EB_id=EB_id,weights=weight_mask) \n",
    "        \n",
    "        print num,EB_id[2:], ECCO.Area_Lake_and_Islands(lake_cart),pxnum,etime,ypix,xpix\n",
    "        \n",
    "    print 'End of function'\n",
    "    return #alist, blist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(412, 424)\n",
      "hdf weights file exists, removing it...\n"
     ]
    }
   ],
   "source": [
    "#ncfile= '/uio/kant/geo-metos-u1/blaken/datadisk/ECCO/CORDEX/Data_CORDEX/tas_EUR-11_ICHEC-EC-EARTH_historical_r1i1p1_KNMI-RACMO22E_v1_day_19660101-19701231.nc'\n",
    "ncfile ='CORDEX/tas_EUR-11_ICHEC-EC-EARTH_rcp45_r1i1p1_KNMI-RACMO22E_v1_day_20960101-21001231.nc'\n",
    "\n",
    "namelist,sumlist = MT_Gen_SWeights(nc_path=ncfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
